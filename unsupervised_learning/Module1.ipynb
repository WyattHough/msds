{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09d2eb5fbe70bea445a5bdf28ac0697b",
     "grade": false,
     "grade_id": "cell-e785bf541c3c6566",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Homework 1: PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50ce8f8109224a024605fa44f6a5958b",
     "grade": false,
     "grade_id": "cell-b5f4e240125e48e2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Problem 1 - Principal Component Analysis\n",
    "---\n",
    "\n",
    "In this problem you'll be implementing Dimensionality reduction using Principal Component Analysis technique. \n",
    "\n",
    "The gist of PCA Algorithm to compute principal components is follows:\n",
    "- Calculate the covariance matrix X of data points.\n",
    "- Calculate eigenvectors and corresponding eigenvalues.\n",
    "- Sort the eigenvectors according to their eigenvalues in decreasing order.\n",
    "- Choose first k eigenvectors which satisfies target explained variance.\n",
    "- Transform the original data of shape m observations times n features into m observations times k selected features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f4571ea879ea92d09a2831fcdac74887",
     "grade": false,
     "grade_id": "cell-eea5ece608ed4ac5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The skeleton for the *PCA* class is below. Scroll down to find more information about your tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43c4c380877a398b95cc910c3e3f0244",
     "grade": false,
     "grade_id": "cell-0e7782486f935045",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a709a717c9fd199ede5881a38c151b97",
     "grade": false,
     "grade_id": "cell-5874a755dcce55e2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    def __init__(self, target_explained_variance=None):\n",
    "        \"\"\"\n",
    "        explained_variance: float, the target level of explained variance\n",
    "        \"\"\"\n",
    "        self.target_explained_variance = target_explained_variance\n",
    "        self.feature_size = -1\n",
    "\n",
    "    def standardize(self, X):\n",
    "        \"\"\"\n",
    "        standardize features using standard scaler\n",
    "        :param X: input data with shape m (# of observations) X n (# of features)\n",
    "        :return: standardized features (Hint: use skleanr's StandardScaler. Import any library as needed)\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        scaler = StandardScaler()\n",
    "        return scaler.fit_transform(X)\n",
    "        \n",
    "\n",
    "    def compute_mean_vector(self, X_std):\n",
    "        \"\"\"\n",
    "        compute mean vector\n",
    "        :param X_std: transformed data\n",
    "        :return n X 1 matrix: mean vector\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        return np.mean(X_std, axis=0)\n",
    "        \n",
    "\n",
    "    def compute_cov(self, X_std, mean_vec):\n",
    "        \"\"\"\n",
    "        Covariance using mean, (don't use any numpy.cov)\n",
    "        :param X_std:\n",
    "        :param mean_vec:\n",
    "        :return n X n matrix:: covariance matrix\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        mat = np.subtract(X_std, mean_vec)\n",
    "        cov = np.dot(mat.T, mat) / (len(X_std) - 1)\n",
    "        return cov\n",
    "        \n",
    "\n",
    "    def compute_eigen_vector(self, cov_mat):\n",
    "        \"\"\"\n",
    "        Eigenvector and eigen values using numpy. Uses numpy's eigenvalue function\n",
    "        :param cov_mat:\n",
    "        :return: (eigen_values, eigen_vector)\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        return np.linalg.eig(cov_mat)\n",
    "        \n",
    "\n",
    "    def compute_explained_variance(self, eigen_vals):\n",
    "        \"\"\"\n",
    "        sort eigen values and compute explained variance.\n",
    "        explained variance informs the amount of information (variance)\n",
    "        can be attributed to each of  the principal components.\n",
    "        :param eigen_vals:\n",
    "        :return: explained variance.\n",
    "        \"\"\"\n",
    "        # Referenced https://vitalflux.com/pca-explained-variance-concept-python-example/\n",
    "        # your code here\n",
    "        sorted_eigen = sorted(eigen_vals, reverse=True)\n",
    "        total = sum(eigen_vals)\n",
    "        explained_variance = [i/total for i in sorted_eigen]\n",
    "        return explained_variance\n",
    "        \n",
    "\n",
    "    def cumulative_sum(self, var_exp):\n",
    "        \"\"\"\n",
    "        return cumulative sum of explained variance.\n",
    "        :param var_exp: explained variance\n",
    "        :return: cumulative explained variance\n",
    "        \"\"\"\n",
    "        return np.cumsum(var_exp)\n",
    "\n",
    "    def compute_weight_matrix(self, eig_pairs, cum_var_exp):\n",
    "        \"\"\"\n",
    "        compute weight matrix of top principal components conditioned on target\n",
    "        explained variance.\n",
    "        (Hint : use cumilative explained variance and target_explained_variance to find\n",
    "        top components)\n",
    "        \n",
    "        :param eig_pairs: list of tuples containing eigenvalues and eigenvectors, \n",
    "        sorted by eigenvalues in descending order (the biggest eigenvalue and corresponding eigenvectors first).\n",
    "        :param cum_var_exp: cumulative expalined variance by features\n",
    "        :return: weight matrix (the shape of the weight matrix is n X k)\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        matrix_w = np.array([eig_pairs[i][1] for i in range(self.feature_size) if cum_var_exp[i] >= self.target_explained_variance])\n",
    "        return matrix_w\n",
    "        \n",
    "\n",
    "    def transform_data(self, X_std, matrix_w):\n",
    "        \"\"\"\n",
    "        transform data to subspace using weight matrix\n",
    "        :param X_std: standardized data\n",
    "        :param matrix_w: weight matrix\n",
    "        :return: data in the subspace\n",
    "        \"\"\"\n",
    "        return X_std.dot(matrix_w)\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"    \n",
    "        entry point to the transform data to k dimensions\n",
    "        standardize and compute weight matrix to transform data.\n",
    "        The fit functioin returns the transformed features. k is the number of features which cumulative \n",
    "        explained variance ratio meets the target_explained_variance.\n",
    "        :param   m X n dimension: train samples\n",
    "        :return  m X k dimension: subspace data. \n",
    "        \"\"\"\n",
    "    \n",
    "        self.feature_size = X.shape[1]\n",
    "        print(f'Size: {self.feature_size}')\n",
    "        \n",
    "        # your code here\n",
    "        X_std = self.standardize(X)\n",
    "        mean_vec = self.compute_mean_vector(X_std)\n",
    "        cov_mat = self.compute_cov(X_std, mean_vec)\n",
    "        eigen_vals, eigen_vecs = self.compute_eigen_vector(cov_mat)\n",
    "        var_exp = self.compute_explained_variance(eigen_vals)\n",
    "        cum_var_exp = self.cumulative_sum(var_exp)\n",
    "        eig_pairs = [(val, vec) for val, vec in zip(eigen_vals, eigen_vecs)]\n",
    "        matrix_w = self.compute_weight_matrix(eig_pairs, cum_var_exp)\n",
    "        #print(matrix_w)\n",
    "        print(len(matrix_w),len(matrix_w[0]))\n",
    "        return self.transform_data(X_std=X_std, matrix_w=matrix_w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d7503bc2e9d44a8932c77600c0b068c",
     "grade": false,
     "grade_id": "cell-72c2a70fcc1b5457",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**[ PART A ]** Your task involves implementing helper functions to compute *mean, covariance, eigenvector and weights*.\n",
    "\n",
    "complete `fit()` to using all helper functions to find reduced dimension data.\n",
    "\n",
    "Run PCA on *fashion mnist dataset* to reduce the dimension of the data.\n",
    "\n",
    "fashion mnist data consists of samples with *784 dimensions*.\n",
    "\n",
    "Report the reduced dimension $k$ for target explained variance of **0.99**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b81e50a1e169900d57f6bde5059b554",
     "grade": false,
     "grade_id": "cell-1ca6638507fdcbd6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_train = pickle.load(open('./data/fashionmnist/train_images.pkl','rb'))\n",
    "y_train = pickle.load(open('./data/fashionmnist/train_image_labels.pkl','rb'))\n",
    "\n",
    "X_train = X_train[:1500]\n",
    "y_train = y_train[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa28b9a88044825ca1e35e05796e6a26",
     "grade": false,
     "grade_id": "cell-7ccdd941a2845fa3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 784\n",
      "375 784\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1500,784) and (375,784) not aligned: 784 (dim 1) != 375 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m pca_handler \u001b[39m=\u001b[39m PCA(target_explained_variance\u001b[39m=\u001b[39m\u001b[39m0.99\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m X_train_updated \u001b[39m=\u001b[39m pca_handler\u001b[39m.\u001b[39;49mfit(X_train)\n",
      "Cell \u001b[0;32mIn[11], line 127\u001b[0m, in \u001b[0;36mPCA.fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39m#print(matrix_w)\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(matrix_w),\u001b[39mlen\u001b[39m(matrix_w[\u001b[39m0\u001b[39m]))\n\u001b[0;32m--> 127\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform_data(X_std\u001b[39m=\u001b[39;49mX_std, matrix_w\u001b[39m=\u001b[39;49mmatrix_w)\n",
      "Cell \u001b[0;32mIn[11], line 101\u001b[0m, in \u001b[0;36mPCA.transform_data\u001b[0;34m(self, X_std, matrix_w)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform_data\u001b[39m(\u001b[39mself\u001b[39m, X_std, matrix_w):\n\u001b[1;32m     95\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39m    transform data to subspace using weight matrix\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[39m    :param X_std: standardized data\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m    :param matrix_w: weight matrix\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    :return: data in the subspace\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     \u001b[39mreturn\u001b[39;00m X_std\u001b[39m.\u001b[39;49mdot(matrix_w)\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1500,784) and (375,784) not aligned: 784 (dim 1) != 375 (dim 0)"
     ]
    }
   ],
   "source": [
    "pca_handler = PCA(target_explained_variance=0.99)\n",
    "X_train_updated = pca_handler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('msds')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "43eafd745181b4decb35f1f883f10f7d33496fe93e40652adc5bf64b8e5cb0c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
