{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wine Quality Project\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This project details the steps taken to create a classifier for predicting the quality of a wine given a set of measurement from the wine. The algorithm implemented will be a support vector machine but we will explore the results of other types of models as well. This model will be adapted to determine if a wine is high quality or not in a binary classification mode. It could be adapted to be a multiclass classifier as well.\n",
    "\n",
    "The goal of this project is to create a wine predictor that will allow people to input data that can be measured from the wine and determine is it is a high quality wine. The quality score used in training were taken from the median score given by 3 different wine experts. People may have different opionions about the taste of a wine but this model could be uses to  determine the best price of a wine and differentiate it from other similar wines. Using this model could be a cheaper alternative to bringing in wine experts to judge a new wine type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The data was taken from the UCI Machine Learning Repositry Wine Quality Data Set (https://archive.ics.uci.edu/ml/datasets/Wine+Quality). The target attribute is the `quality` field and we will use feature selection to narrow down the other attributes. There are 6497 total wine samples, a detailed description of the data can be seen in the \"winequality.names\" file included in this project folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red = pd.read_csv('winequality-red.csv', delimiter=';')\n",
    "print(red.dtypes)\n",
    "red.quality.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white = pd.read_csv('winequality-white.csv', delimiter=';')\n",
    "print(white.dtypes)\n",
    "white.quality.hist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice in both of these datasets there are not many samples of high quality or low quality wines, most are in the middle. For our case, we will split the quality to be any value 7 or above will be high quality and any quality of 6 or below will not. We will keep these data sets separate for now since there may be different things that determine quality between red and white wines but we will experiment with combining them later on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning\n",
    "\n",
    "We will need to renencode the quality field to be a binary class of 1 for high quality or 0 for not high quality. We will also check if there are any null values in the data that need to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Null Values: {red.isna().sum().sum() + white.isna().sum().sum()}\")\n",
    "\n",
    "# Set value of quality to 1 if score was 7 or greater, 0 otherwise\n",
    "red.quality = np.where(red.quality >= 7, 1, 0)\n",
    "white.quality = np.where(white.quality >= 7, 1, 0)\n",
    "\n",
    "red_x = red.drop('quality', axis=1)\n",
    "red_y = red.quality\n",
    "white_x = white.drop('quality', axis=1)\n",
    "white_y = white.quality\n",
    "print(red_y.sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis\n",
    "\n",
    "In this section we will inspect the data and perform principle component analysis to reduce the number of features in our data.\n",
    "\n",
    "We notice from the correlation matrix that the red wine has a high negative correlation between `pH`, `fixed acidity`, and `citric acid`. This is unsurprising since all of these are related to the acidic levels of the wine. There is also a strong positive correlation between `fixed acidity` and `density` as well as `free sulfur dioxide` and `total sulfur dioxide`. Therefore we will drop the `fixed acidity`, `citric acid`, and `free sulfur dioxide` fields.\n",
    "\n",
    "For the white wine we see from the correlation matrix that there is a correlation between `alcohol` and `density` as well as `density` and `pH`. Similar to the red wines there is also correlation between `free sulfur dioxide` and `total sulfur dioxide`. Thus we will drop `density` and `free sulfur dioxide`. This is a surprising result as I would have expected a similar set of correlated features between the two types of wines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Red data set\n",
    "sns.heatmap(red_x.corr())\n",
    "sns.pairplot(red_x, diag_kind='kde')\n",
    "red_x.drop(['fixed acidity', 'citric acid', 'free sulfur dioxide'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore White data set\n",
    "sns.heatmap(white_x.corr())\n",
    "sns.pairplot(white_x, diag_kind='kde')\n",
    "white_x.drop(['density', 'free sulfur dioxide'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "The model used is a support vector machine for multi class classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(kernel='linear')\n",
    "svc.fit(red_x, red_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "\n",
    "params = {\n",
    "    \"C\": np.logspace(-5, 5, num=10, base=2),\n",
    "    \"gamma\": np.logspace(-5, 5, num=10, base=2),\n",
    "    #\"kernel\": [\"rbf\", \"sigmoid\"]\n",
    "    #\"degree\": range(0, 10)\n",
    "}\n",
    "red_grid = GridSearchCV(SVC(), param_grid=params, cv=3)\n",
    "red_grid.fit(red_x, red_y)\n",
    "\n",
    "print(red_grid.best_params_)\n",
    "print(red_grid.best_score_)\n",
    "\n",
    "#linear\n",
    "# {'C': 0.03125, 'gamma': 0.03125}\n",
    "# 0.864290181363352\n",
    "\n",
    "# {'C': 1.4697344922755986, 'gamma': 0.3149802624737183, 'kernel': 'rbf'}\n",
    "# 0.8667917448405253"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import Normalize\n",
    "\n",
    "scores = [x for x in red_grid.cv_results_[\"mean_test_score\"]]\n",
    "scores = np.array(scores).reshape(len(red_grid.param_grid[\"C\"]), len(red_grid.param_grid[\"gamma\"]))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\n",
    "plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot,\n",
    "            norm=Normalize(vmin=0.2, vmax=0.92))\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('C')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(red_grid.param_grid[\"gamma\"])), red_grid.param_grid[\"gamma\"], rotation=45)\n",
    "plt.yticks(np.arange(len(red_grid.param_grid[\"C\"])), red_grid.param_grid[\"C\"])\n",
    "plt.title('Validation accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results and Analysis\n",
    "\n",
    "I ran through several iterations of GridSearch to implement hyperparameter tuning and cross validation to help select the best model. Unforunutelt each training run took a long time to complete so I was only able to iterate over a few different combinations. I found that linear performed better than polynomial and a C value of 32 was considered the best in all the models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "The model that was created was the result of hyperparameter tuning and domain knowledge. The data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f452f41d5c38245089c316480f67122644a5c4908ac30fbe8afd16088853e9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
